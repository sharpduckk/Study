{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# ### Tiny Imagenet Visual Recognition Challenge\n",
    "# Tiny Imagenet has 200 Classes, each class has 500 traininig images, 50 Validation Images and 50 test images. Label Classes and Bounding Boxes are provided. More details can be found at https://tiny-imagenet.herokuapp.com/\n",
    "# This challenge is part of Stanford Class CS 213N\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn import preprocessing\n",
    "# jupyter path = \"/home/jupyter/notebooks/python35\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "NUM_CLASSES = 200\n",
    "NUM_IMAGES_PER_CLASS = 500\n",
    "# NUM_IMAGES = NUM_CLASSES * NUM_IMAGES_PER_CLASS\n",
    "NUM_IMAGES = 98179\n",
    "TRAINING_IMAGES_DIR = \"/data1/tiny-imagenet-200/train/\"\n",
    "# TRAINING_IMAGES_DIR = './tiny-imagenet-200/train/'\n",
    "TRAIN_SIZE = NUM_IMAGES\n",
    "\n",
    "NUM_VAL_IMAGES = 9832\n",
    "# VAL_IMAGES_DIR = './tiny-imagenet-200/val/'\n",
    "VAL_IMAGES_DIR = \"/data1/tiny-imagenet-200/val/\"\n",
    "IMAGE_SIZE = 64\n",
    "NUM_CHANNELS = 3\n",
    "IMAGE_ARR_SIZE = IMAGE_SIZE * IMAGE_SIZE * NUM_CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/jupyter/notebook/python35/duck'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_images(image_dir, batch_size=500):\n",
    "    image_index = 0\n",
    "\n",
    "    images = np.ndarray(shape=(NUM_IMAGES, IMAGE_ARR_SIZE))\n",
    "    names = []\n",
    "    labels = []\n",
    "\n",
    "    print(\"Loading training images from \", image_dir)\n",
    "    # Loop through all the types directories\n",
    "    for type in os.listdir(image_dir):\n",
    "        if os.path.isdir(image_dir + type + '/images/'):\n",
    "            type_images = os.listdir(image_dir + type + '/images/')\n",
    "            # print(type_images)\n",
    "            # Loop through all the images of a type directory\n",
    "            batch_index = 0;\n",
    "            # print (\"Loading Class \", type)\n",
    "            for image in type_images:\n",
    "                image_file = os.path.join(image_dir, type + '/images/', image)\n",
    "                print(image_file)\n",
    "\n",
    "                # reading the images as they are; no normalization, no color editing\n",
    "                image_data = mpimg.imread(image_file)\n",
    "                # print ('Loaded Image', image_file, image_data.shape)\n",
    "                if (image_data.shape == (IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)):\n",
    "                    images[image_index, :] = image_data.flatten()\n",
    "\n",
    "                    labels.append(type)\n",
    "                    names.append(image)\n",
    "\n",
    "                    image_index += 1\n",
    "                    batch_index += 1\n",
    "                if (batch_index >= batch_size):\n",
    "                    break;\n",
    "\n",
    "    print(\"Loaded Training Images\", image_index)\n",
    "    return (images, np.asarray(labels), np.asarray(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_name(data, name):\n",
    "    for idx, row in data.iterrows():\n",
    "        if (row['File'] == name):\n",
    "            return row['Class']\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_validation_images(testdir, validation_data, batch_size=NUM_VAL_IMAGES):\n",
    "    labels = []\n",
    "    names = []\n",
    "    image_index = 0\n",
    "\n",
    "    images = np.ndarray(shape=(batch_size, IMAGE_ARR_SIZE))\n",
    "    val_images = os.listdir(testdir + '/images/')\n",
    "\n",
    "    # Loop through all the images of a val directory\n",
    "    batch_index = 0;\n",
    "\n",
    "    for image in val_images:\n",
    "        image_file = os.path.join(testdir, 'images/', image)\n",
    "        print (\"path: \", testdir, image_file)\n",
    "\n",
    "        # reading the images as they are; no normalization, no color editing\n",
    "        image_data = mpimg.imread(image_file)\n",
    "        if (image_data.shape == (IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)):\n",
    "            images[image_index, :] = image_data.flatten()\n",
    "            image_index += 1\n",
    "            labels.append(get_label_from_name(validation_data, image))\n",
    "            names.append(image)\n",
    "            batch_index += 1\n",
    "\n",
    "        if (batch_index >= batch_size):\n",
    "            break;\n",
    "\n",
    "    print(\"Loaded Validation images \", image_index)\n",
    "    return (images, np.asarray(labels), np.asarray(names))\n",
    "\n",
    "\n",
    "def get_next_batch(batchsize=50):\n",
    "    for cursor in range(0, len(training_images), batchsize):\n",
    "        batch = []\n",
    "        batch.append(training_images[cursor:cursor + batchsize])\n",
    "        batch.append(training_labels_encoded[cursor:cursor + batchsize])\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images, training_labels, training_files = load_training_images(TRAINING_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['n04070727', 'n02190166', 'n07747607', ..., 'n01644900',\n",
       "       'n02403003', 'n02999410'], dtype='<U9')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('training_images', training_images)\n",
    "# np.save('training_labels', training_labels)\n",
    "# np.save('training_files', training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_images = np.load('training_images.npy')\n",
    "# training_labels = np.load('training_labels.npy')\n",
    "# training_files = np.load('training_files.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98179, 12288)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98179,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 Training Labels [ 56  57   0  75  62  10 130  85  95 174 187 188  35  74  79  86 101 145\n",
      "  48  31  84 107  43  33  65  22  72   4  35  44]\n"
     ]
    }
   ],
   "source": [
    "shuffle_index = np.random.permutation(len(training_labels))\n",
    "training_images = training_images[shuffle_index]\n",
    "training_labels = training_labels[shuffle_index]\n",
    "training_files = training_files[shuffle_index]\n",
    "\n",
    "le_1 = preprocessing.LabelEncoder() #\n",
    "training_le = le_1.fit(training_labels) #\n",
    "training_labels_encoded = training_le.transform(training_labels)\n",
    "print(\"First 30 Training Labels\", training_labels_encoded[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(VAL_IMAGES_DIR + 'val_annotations.txt', sep='\\t', header=None,\n",
    "                       names=['File', 'Class', 'X', 'Y', 'H', 'W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "val_images, val_labels, val_files = load_validation_images(VAL_IMAGES_DIR, val_data, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('val_images', val_images)\n",
    "np.save('val_labels', val_labels)\n",
    "np.save('val_files', val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = np.load('val_images.npy')\n",
    "val_labels = np.load('val_labels.npy')\n",
    "val_files = np.load('val_files.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165  42  58   1   7 174  33   9 176  43 155 184  13  34  66  36 180 108\n",
      " 165 103  79 176  90 134 136  38 180 165  94 107]\n"
     ]
    }
   ],
   "source": [
    "#le_2 = preprocessing.LabelEncoder() #\n",
    "#val_le = le_2.fit(training_labels) #\n",
    "#val_labels_encoded = val_le.transform(val_labels)\n",
    "val_labels_encoded = training_le.transform(val_labels)\n",
    "print(val_labels_encoded[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9832"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12288"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = IMAGE_SIZE\n",
    "width = IMAGE_SIZE\n",
    "channels = NUM_CHANNELS\n",
    "n_inputs = height * width * channels\n",
    "n_outputs = 200\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "# input shape [-1, 64, 64, 3]\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=X_reshaped,\n",
    "    filters=32,\n",
    "    kernel_size=[5, 5],\n",
    "    padding='SAME',\n",
    "    activation=tf.nn.relu,\n",
    "    name=\"conv1\")\n",
    "\n",
    "# shape after conv1: [-1, 64, 64, 32]\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=64,\n",
    "    kernel_size=[5, 5],\n",
    "    padding='SAME',\n",
    "    activation=tf.nn.relu,\n",
    "    name=\"conv2\")\n",
    "\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Dense Layer\n",
    "pool2_flat = tf.reshape(pool2, [-1, 8 * 8 * 64])\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout = tf.layers.dropout(inputs=dense, rate=0.4)\n",
    "dropout_reshape = tf.reshape(dropout, [-1, 8 * 8 * 64])\n",
    "\n",
    "# Logits Layer\n",
    "logits = tf.layers.dense(inputs=dropout_reshape, units=200, name='output')\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.03448276 Test accuracy: 0.0074247355\n",
      "1 Train accuracy: 0.06896552 Test accuracy: 0.00864524\n",
      "2 Train accuracy: 0.06896552 Test accuracy: 0.009764036\n",
      "3 Train accuracy: 0.03448276 Test accuracy: 0.016985353\n",
      "4 Train accuracy: 0.0 Test accuracy: 0.013832384\n",
      "5 Train accuracy: 0.03448276 Test accuracy: 0.020545159\n",
      "6 Train accuracy: 0.03448276 Test accuracy: 0.022172498\n",
      "7 Train accuracy: 0.03448276 Test accuracy: 0.026139138\n",
      "8 Train accuracy: 0.03448276 Test accuracy: 0.026240846\n",
      "9 Train accuracy: 0.03448276 Test accuracy: 0.026647681\n",
      "10 Train accuracy: 0.03448276 Test accuracy: 0.030309195\n",
      "11 Train accuracy: 0.03448276 Test accuracy: 0.028071603\n",
      "12 Train accuracy: 0.06896552 Test accuracy: 0.027359642\n",
      "13 Train accuracy: 0.03448276 Test accuracy: 0.029190399\n",
      "14 Train accuracy: 0.03448276 Test accuracy: 0.03000407\n",
      "15 Train accuracy: 0.06896552 Test accuracy: 0.02990236\n",
      "16 Train accuracy: 0.10344828 Test accuracy: 0.028071603\n",
      "17 Train accuracy: 0.13793103 Test accuracy: 0.027257932\n",
      "18 Train accuracy: 0.13793103 Test accuracy: 0.02990236\n",
      "19 Train accuracy: 0.20689656 Test accuracy: 0.030410903\n",
      "20 Train accuracy: 0.1724138 Test accuracy: 0.031224573\n",
      "21 Train accuracy: 0.1724138 Test accuracy: 0.02746135\n",
      "22 Train accuracy: 0.13793103 Test accuracy: 0.030919448\n",
      "23 Train accuracy: 0.13793103 Test accuracy: 0.0315297\n",
      "24 Train accuracy: 0.1724138 Test accuracy: 0.028885273\n",
      "25 Train accuracy: 0.1724138 Test accuracy: 0.03061432\n",
      "26 Train accuracy: 0.20689656 Test accuracy: 0.030207485\n",
      "27 Train accuracy: 0.13793103 Test accuracy: 0.028376728\n",
      "28 Train accuracy: 0.13793103 Test accuracy: 0.027359642\n",
      "29 Train accuracy: 0.20689656 Test accuracy: 0.029292107\n",
      "30 Train accuracy: 0.2413793 Test accuracy: 0.028071603\n",
      "31 Train accuracy: 0.2413793 Test accuracy: 0.031834826\n",
      "32 Train accuracy: 0.20689656 Test accuracy: 0.03203824\n",
      "33 Train accuracy: 0.1724138 Test accuracy: 0.029393816\n",
      "34 Train accuracy: 0.27586207 Test accuracy: 0.02756306\n",
      "35 Train accuracy: 0.2413793 Test accuracy: 0.028986981\n",
      "36 Train accuracy: 0.27586207 Test accuracy: 0.03071603\n",
      "37 Train accuracy: 0.2413793 Test accuracy: 0.027969895\n",
      "38 Train accuracy: 0.2413793 Test accuracy: 0.03000407\n",
      "39 Train accuracy: 0.20689656 Test accuracy: 0.02522376\n",
      "40 Train accuracy: 0.1724138 Test accuracy: 0.026952807\n",
      "41 Train accuracy: 0.27586207 Test accuracy: 0.026851099\n",
      "42 Train accuracy: 0.31034482 Test accuracy: 0.02746135\n",
      "43 Train accuracy: 0.1724138 Test accuracy: 0.030309195\n",
      "44 Train accuracy: 0.20689656 Test accuracy: 0.02593572\n",
      "45 Train accuracy: 0.2413793 Test accuracy: 0.028681856\n",
      "46 Train accuracy: 0.27586207 Test accuracy: 0.026545972\n",
      "47 Train accuracy: 0.1724138 Test accuracy: 0.028071603\n",
      "48 Train accuracy: 0.20689656 Test accuracy: 0.027766477\n",
      "49 Train accuracy: 0.2413793 Test accuracy: 0.029800652\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in get_next_batch():\n",
    "            X_batch, y_batch = batch[0], batch[1]\n",
    "            # print ('Training set', X_batch.shape, y_batch.shape)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: val_images, y: val_labels_encoded})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        save_path = saver.save(sess, \"./tiny_imagenet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
